IKAnalyzer 是一个开源的，基于java语言开发的轻量级的中文分词工具包最初，它是以开源项目 Lucene为应用主体的，结合词典分词和文法分析算法的中文分词组件。新版本的IKAnalyzer3.0则发展为 面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。

## Lucene还有 IK 要解决一个什么问题，以及大致的原理？ 
我们传统的数据库是把文本整段存在库里面，当是搜索一个词的时候，它使用这个词去大文本里面去匹配，这个匹配过程速度很慢。Lucene是一个全文搜索引擎，它的思路是倒过来的。它做的事情分成两个阶段。第一阶段是把文章，也就是长文本在存储的时候建索引。建索引的过程要把文本按照词来切分。切分的时候是按词源切分，再把每个词都作为索引，指出词在文章中的位置以及跟哪些文章相关联。当有大量文章进来的时候，你可以想象一下有一个字典，这个字典是按照词来排序，每个词都会记录有哪些文章引用到我了，以及文章的哪些部分。这个索引的过程就是Lucene做的一个存储过程。那在这个存储过程中，对于英文而言基本上按照空格来切分就可以了，但是中文是连续性的，所以计算机就需要一个能够识别中文词汇的程序和算法，这就是中文分词器。分词器在文章进入到数据库的时候，对它进行分词。比如用户输入了“咖啡牛排”，那分词器就要把这个切分成“咖啡”和“牛排”。

## 分词的基本原理？ 
分词器并不具备人工智能的能力，所以它所做的就是把文本和词典中的词进行比较，按照词典中已有的词进行匹配，匹配完成之后即达到了分词的效果。
将一段文字进行IK分词处理一般经过：词典加载、预处理、分词器分词、歧义处理、善后结尾 五个部分。

### 词典加载：
主程序需要加载词典。IK分词的词典主要包括 main2012.dic(主词典)、quantifier.dic(量词)、stopword.dic(停用词)、ext.dic(扩展词,可选)四个字典。字典的结构使用字典树进行存储。关于字典树的具体介绍可以参考： http://www.cnblogs.com/rush/archive/2012/12/30/2839996.html
### 预处理：
预处理是在加载文档的时候做的，其实预处理部分只是做两件事情：
- 识别出每个字符的字符类型
- 字符转化：全角转半角，大写转小写

字符类型是在分词的时候需要，不同的分词器会根据不同的字符类型做特别处理。而字符转化则是要求文本字符与词典中的字符想匹配，比如对于12288、32这两个字符，分别是汉字和英文的空格，如果一篇文本中包含了汉字包含了这两个字符，那么分词器在分词的时候需要判断两个都是空格。同理在英文大小写上也存在类似的问题。预处理主要在org.wltea.analyzer.core.CharacterUtil.java类中实现。
### 分词器分词：
IK分词包括三个分词器：LetterSegmenter（字符分词器）,CN_QuantifierSegmenter（中文数量词分词器）,CJKSegmenter（中日韩文分词器）。分词器分词有两种模式，即：smart模式和非smart模式，非smart模式可以认为是最小力度的分词，smart模式则反之。
具体的实例：
```
张三说的确实在理

smart模式的下分词结果为：  
张三 | 说的 | 确实 | 在理

而非smart模式下的分词结果为：
张三 | 三 | 说的 | 的确 | 的 | 确实 | 实在 | 在理
```

IK分词使用了”正向迭代最细粒度切分算法“，简单说来就是：</br> Segmenter会逐字识别词元，设输入”中华人民共和国“并且”中“单个字也是字典里的一个词，那么过程是这样的：”中“是词元也是前缀（因为有各种中开头的词），加入词元”中“；继续下一个词”华“，由于中是前缀，那么可以识别出”中华“，同时”中华“也是前缀因此加入”中华“词元，并把其作为前缀继续；接下来继续发现“华人”是词元，“中华人”是前缀，以此类推……</br>
关于IK分词的三个分词器，基本上逻辑是一样的，如果有兴趣了解的话，重点看一下CJKSegmenter就好了。

### 歧义处理：
非smart模式所做的就是将能够分出来的词全部输出；smart模式下，IK分词器则会根据内在方法输出一个认为最合理的分词结果，这就涉及到了歧义判断。
以“张三说的确实在理”为依据，分词的结果为：
```
张三 | 三 | 说的 | 的确 | 的 | 确实 | 实在 | 在理 
```
在判断歧义的时候首先要看词与词之间是否用冲突，“张三”与“三”相冲突，“说的”，“的确”，“的”，“确实”，“实在”，“在理”相冲突，所以分成两部分进行歧义判断。我们以后一种为例：“说的”，“的确”，“的”，“确实”，“实在”， 按照默认的生成的一组结果为“说的|确实|在理”，与这个结果相冲突的词是“的确”、“的”、“实在”。回滚生成的结果，将这三个词倒叙逐一放入到生成的结果中，生成可选分词方案是:
```
"实在"放入后，分词方案为："说的|实在|";
"的"放入后，生成的分词方案为："的|确实|在理";
"的确"放入后，生成的分词方案为："的确|实在";
```
这样加上原有的一共四种可选的分词方案，从中选取最优的方案。选取的原则顺序如下：
- 1、比较有效文本长度  
- 2、比较词元个数，越少越好  
- 3、路径跨度越大越好  
- 4、最后一个词元的位置越靠后约好（根据统计学结论，逆向切分概率高于正向切分，因此位置越靠后的优先）  
- 5、词长越平均越好  
- 6、词元位置权重比较  

> 总体说来就是： 主要使用的就是贪心算法获取局部最优解，然后继续处理来获取最终的结果。
### 善后结尾：
处理遗漏中文字符，遍历输入文本，把词之间每个中文字符也作为词输出，虽然词典中没有这样的词。
处理数量词，如果中文量词刚好出现在中文数词的后面，或者中文量词刚好出现在阿拉伯数字的后面，则把数词和量词合并。比如"九寸""十二亩"。